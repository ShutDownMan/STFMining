{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./CSV/train_small.csv\")\n",
    "\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  themes process_id                            file_name document_type  pages  \\\n",
       "0  [232]  AI_856934  AI_856934_1926210_1060_17072013.pdf        outros      1   \n",
       "1  [232]  AI_856934    AI_856934_1926211_34_17072013.pdf        outros      1   \n",
       "2  [232]  AI_856934    AI_856934_1926211_34_17072013.pdf        outros      2   \n",
       "3  [232]  AI_856934    AI_856934_1926211_34_17072013.pdf        outros      3   \n",
       "4  [232]  AI_856934    AI_856934_1926211_34_17072013.pdf        outros      4   \n",
       "\n",
       "                                                body  \n",
       "0  {\"tribunal justiça estado bahia poder judiciár...  \n",
       "1  {\"excelentíssimo senhor doutor juiz direito ju...  \n",
       "2  {\"razões recurso inominado recorrente atlantic...  \n",
       "3  {\"empresa recorrente tornou credora dos débito...  \n",
       "4  {\"entretanto verdade parte apelante tornou tit...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>themes</th>\n",
       "      <th>process_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>document_type</th>\n",
       "      <th>pages</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[232]</td>\n",
       "      <td>AI_856934</td>\n",
       "      <td>AI_856934_1926210_1060_17072013.pdf</td>\n",
       "      <td>outros</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"tribunal justiça estado bahia poder judiciár...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[232]</td>\n",
       "      <td>AI_856934</td>\n",
       "      <td>AI_856934_1926211_34_17072013.pdf</td>\n",
       "      <td>outros</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"excelentíssimo senhor doutor juiz direito ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[232]</td>\n",
       "      <td>AI_856934</td>\n",
       "      <td>AI_856934_1926211_34_17072013.pdf</td>\n",
       "      <td>outros</td>\n",
       "      <td>2</td>\n",
       "      <td>{\"razões recurso inominado recorrente atlantic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[232]</td>\n",
       "      <td>AI_856934</td>\n",
       "      <td>AI_856934_1926211_34_17072013.pdf</td>\n",
       "      <td>outros</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"empresa recorrente tornou credora dos débito...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[232]</td>\n",
       "      <td>AI_856934</td>\n",
       "      <td>AI_856934_1926211_34_17072013.pdf</td>\n",
       "      <td>outros</td>\n",
       "      <td>4</td>\n",
       "      <td>{\"entretanto verdade parte apelante tornou tit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df.head().iloc[0]['body']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"tribunal justiça estado bahia poder judiciário salvador juizado cível defesa consumidor universo matutino projudi acm campus universidade salgado oliveira universo pituba salvador processo parte autora nailton lantyer cordeiro araujo parte atlantico fundo investimento direitos creditorios despacho vistos etc indefiro requerido pela parte demandante aguarde a sessão conciliação designada salvador de julho paulo alberto nunes chenaud juiz direito documento assinado eletronicamente\"}'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df['document_type'].unique()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['outros', 'sentenca', 'peticao_do_RE',\n",
       "       'despacho_de_admissibilidade', 'acordao_de_2_instancia',\n",
       "       'agravo_em_recurso_extraordinario'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# df['themes'].unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# import nltk\n",
    "\n",
    "# word_data = df.head().iloc[0]['body']\n",
    "\n",
    "# nltk_tokens = nltk.word_tokenize(word_data, language='portuguese')\n",
    "\n",
    "# nltk_tokens = [word for word in nltk_tokens if len(word) > 3]\n",
    "\n",
    "# smushed = ' '.join(nltk_tokens)\n",
    "\n",
    "# # nltk_tokens\n",
    "# # smushed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# creating text dataframe\n",
    "text_df = pd.DataFrame(df[['body','themes']])\n",
    "\n",
    "with tqdm(total=text_df.shape[0]*2) as pbar:\n",
    "    # tokenizing words by whitespace\n",
    "    text_df['body'] = [nltk.word_tokenize(word_data, language='portuguese') for word_data in text_df['body']]\n",
    "\n",
    "    pbar.update(text_df.shape[0])\n",
    "\n",
    "    # removing words with length < 3\n",
    "    def remove_small_words(tokens):\n",
    "        pbar.update(1)\n",
    "        return [word for word in tokens if len(word) > 3]\n",
    "\n",
    "    text_df['body'] = [remove_small_words(tokens) for tokens in text_df['body']]\n",
    "\n",
    "text_df"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 298434/298434 [01:26<00:00, 3455.57it/s] \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                     body      themes\n",
       "0       [tribunal, justiça, estado, bahia, poder, judi...       [232]\n",
       "1       [excelentíssimo, senhor, doutor, juiz, direito...       [232]\n",
       "2       [razões, recurso, inominado, recorrente, atlan...       [232]\n",
       "3       [empresa, recorrente, tornou, credora, débitos...       [232]\n",
       "4       [entretanto, verdade, parte, apelante, tornou,...       [232]\n",
       "...                                                   ...         ...\n",
       "149212  [supremo, tribunal, federal, recurso, extraord...  [313, 334]\n",
       "149213  [seção, recursos, extraordinários, mandado, in...  [313, 334]\n",
       "149214  [ttar, qsvòwi, edewrr, seção, recursos, extrao...  [313, 334]\n",
       "149215  [ertidao, certifico, dirigi, setor, autarquias...  [313, 334]\n",
       "149216  [supremo, tribunal, federal, secretaria, judic...  [313, 334]\n",
       "\n",
       "[149217 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>themes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tribunal, justiça, estado, bahia, poder, judi...</td>\n",
       "      <td>[232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[excelentíssimo, senhor, doutor, juiz, direito...</td>\n",
       "      <td>[232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[razões, recurso, inominado, recorrente, atlan...</td>\n",
       "      <td>[232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[empresa, recorrente, tornou, credora, débitos...</td>\n",
       "      <td>[232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[entretanto, verdade, parte, apelante, tornou,...</td>\n",
       "      <td>[232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149212</th>\n",
       "      <td>[supremo, tribunal, federal, recurso, extraord...</td>\n",
       "      <td>[313, 334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149213</th>\n",
       "      <td>[seção, recursos, extraordinários, mandado, in...</td>\n",
       "      <td>[313, 334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149214</th>\n",
       "      <td>[ttar, qsvòwi, edewrr, seção, recursos, extrao...</td>\n",
       "      <td>[313, 334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149215</th>\n",
       "      <td>[ertidao, certifico, dirigi, setor, autarquias...</td>\n",
       "      <td>[313, 334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149216</th>\n",
       "      <td>[supremo, tribunal, federal, secretaria, judic...</td>\n",
       "      <td>[313, 334]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149217 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# spell = SpellChecker(language='pt')\n",
    "# unknowns = spell.unknown(nltk_tokens)\n",
    "\n",
    "# nltk_tokens = [spell.correction(word) for word in nltk_tokens]\n",
    "# nltk_tokens = [spell.correction(word) for word in nltk_tokens if word not in unknowns]\n",
    "\n",
    "# # nltk_tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# spell = SpellChecker(language='pt')\n",
    "\n",
    "# # spellcheck words and remove misspelled \n",
    "# with tqdm(total=text_df.shape[0]) as pbar:\n",
    "#     def spellcheck_tokens(tokens):\n",
    "#         unknowns = spell.unknown(tokens)\n",
    "#         pbar.update(1)\n",
    "#         return [token for token in tokens if token not in unknowns]\n",
    "\n",
    "#     text_df['body'] = [spellcheck_tokens(tokens) for tokens in text_df['body']]\n",
    "\n",
    "# text_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# nltk_tokens = [word for word in nltk_tokens if not word in stopwords]\n",
    "\n",
    "# # nltk_tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# get portuguese stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# stopword removal\n",
    "with tqdm(total=text_df.shape[0]) as pbar:\n",
    "    def remove_stopwords(tokens):\n",
    "        pbar.update(1)\n",
    "        return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    text_df['body'] = [remove_stopwords(tokens) for tokens in text_df['body']]\n",
    "\n",
    "text_df"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 149217/149217 [00:37<00:00, 4010.07it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                     body      themes\n",
       "0       [tribunal, justiça, estado, bahia, poder, judi...       [232]\n",
       "1       [excelentíssimo, senhor, doutor, juiz, direito...       [232]\n",
       "2       [razões, recurso, inominado, recorrente, atlan...       [232]\n",
       "3       [empresa, recorrente, tornou, credora, débitos...       [232]\n",
       "4       [entretanto, verdade, parte, apelante, tornou,...       [232]\n",
       "...                                                   ...         ...\n",
       "149212  [supremo, tribunal, federal, recurso, extraord...  [313, 334]\n",
       "149213  [seção, recursos, extraordinários, mandado, in...  [313, 334]\n",
       "149214  [ttar, qsvòwi, edewrr, seção, recursos, extrao...  [313, 334]\n",
       "149215  [ertidao, certifico, dirigi, setor, autarquias...  [313, 334]\n",
       "149216  [supremo, tribunal, federal, secretaria, judic...  [313, 334]\n",
       "\n",
       "[149217 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>themes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tribunal, justiça, estado, bahia, poder, judi...</td>\n",
       "      <td>[232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[excelentíssimo, senhor, doutor, juiz, direito...</td>\n",
       "      <td>[232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[razões, recurso, inominado, recorrente, atlan...</td>\n",
       "      <td>[232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[empresa, recorrente, tornou, credora, débitos...</td>\n",
       "      <td>[232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[entretanto, verdade, parte, apelante, tornou,...</td>\n",
       "      <td>[232]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149212</th>\n",
       "      <td>[supremo, tribunal, federal, recurso, extraord...</td>\n",
       "      <td>[313, 334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149213</th>\n",
       "      <td>[seção, recursos, extraordinários, mandado, in...</td>\n",
       "      <td>[313, 334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149214</th>\n",
       "      <td>[ttar, qsvòwi, edewrr, seção, recursos, extrao...</td>\n",
       "      <td>[313, 334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149215</th>\n",
       "      <td>[ertidao, certifico, dirigi, setor, autarquias...</td>\n",
       "      <td>[313, 334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149216</th>\n",
       "      <td>[supremo, tribunal, federal, secretaria, judic...</td>\n",
       "      <td>[313, 334]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149217 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# smush all tokens together\n",
    "text_df['body'] = [' '.join(tokens) for tokens in text_df['body']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "text_df.iloc[80000]['body']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'poder judiciário juizado especial federal região subseção judiciária estado paulo paulista bela vista paulo fone processo autor salvador jose santos certidão certifico publicado dezembro expediente correspondente dispositivo termo devidamente disponibilizado diário eletrônico justiça útil anterior publicação resolução comunicado coge francine shiota técnico judiciário paulo dezembro'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import pickle \n",
    "\n",
    "fh = open('./pickled/text_DataFrame.pkl', 'wb')\n",
    "pickle.dump(text_df, fh)\n",
    "fh.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import pickle \n",
    "\n",
    "fh = open('./pickled/text_DataFrame.pkl', 'rb')\n",
    "text_df = pickle.load(fh)\n",
    "fh.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "train_data_X = text_df['body']\n",
    "train_data_y = text_df['themes']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# pipe_nb = Pipeline((\n",
    "#     (\"vectorizer\", TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=True,\n",
    "#                                    min_df=0.1, max_features=100000)),\n",
    "#     (\"clf\", OneVsRestClassifier(MultinomialNB(alpha=0.001, fit_prior=True), n_jobs=-1))\n",
    "# ))\n",
    "\n",
    "# pipe_svc = Pipeline((\n",
    "#     (\"vectorizer\", TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=True,\n",
    "#                                    min_df=0.1, max_features=100000)),\n",
    "#     (\"clf\", OneVsRestClassifier(LinearSVC(verbose=2, class_weight=\"balanced\"), n_jobs=-1))\n",
    "# ))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# pipe_nb.fit(X_train, y_train)\n",
    "\n",
    "# pipe_nb.score(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "# pipe_svc.score(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "1/len(train_data_y.unique())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0026246719160104987"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorizer = TfidfVectorizer(analyzer='word', \n",
    "#                               token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "#                               ngram_range=(1, 1),\n",
    "#                               sublinear_tf=True,\n",
    "#                               min_df=0.1, max_features=10000)\n",
    "\n",
    "# tf_idf = vectorizer.fit_transform(text_df['body'])\n",
    "# tf_idf = pd.DataFrame(vectorized.toarray(), \n",
    "#              index=['sentence '+str(i) \n",
    "#                     for i in range(1, 1+len(text_df['body']))],\n",
    "#              columns=vectorizer.get_feature_names())\n",
    "\n",
    "# tf_idf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "THEMES = [5, 6, 26, 33, 139, 163, 232, 313, 339, 350, 406, 409, 555, 589,\n",
    "          597, 634, 660, 695, 729, 766, 773, 793, 800, 810, 852, 895, 951, 975]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "def transform_y(labels):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(labels)\n",
    "\n",
    "    mlb_labels = mlb.transform(labels)\n",
    "\n",
    "    return mlb_labels, mlb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from ast import literal_eval\n",
    "train_data_y = train_data_y.apply(lambda x: literal_eval(x))\n",
    "train_data_y = train_data_y.apply(lambda x: list(set(sorted([i if i in THEMES else 0 for i in x]))))\n",
    "\n",
    "y_train, mlb = transform_y(train_data_y)\n",
    "\n",
    "print(train_data_y)\n",
    "print('Classes: ', mlb.classes_)\n",
    "print('Classifying {} themes.'.format(y_train.shape[1]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0            [232]\n",
      "1            [232]\n",
      "2            [232]\n",
      "3            [232]\n",
      "4            [232]\n",
      "            ...   \n",
      "149212    [0, 313]\n",
      "149213    [0, 313]\n",
      "149214    [0, 313]\n",
      "149215    [0, 313]\n",
      "149216    [0, 313]\n",
      "Name: themes, Length: 149217, dtype: object\n",
      "Classes:  [  0   5   6  26  33 139 163 232 313 339 350 406 409 555 589 597 634 660\n",
      " 695 729 766 773 793 800 810 852 895 951 975]\n",
      "Classifying 29 themes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "themes = mlb.classes_\n",
    "\n",
    "with tqdm(total=len(themes)) as pbar:\n",
    "\n",
    "    def generate_wc(s_theme):\n",
    "        full_txt = ' '.join(train_data_X[[s_theme in theme for theme in train_data_y]])\n",
    "        create_wordcloud(full_txt, str(s_theme) + '_idf_wc.png')\n",
    "        # pbar.update(1)\n",
    "\n",
    "    def create_wordcloud(text, file_path):\n",
    "        # create wordcloud object\n",
    "        wc = WordCloud(background_color=\"white\", width=1280, height=720, max_words=200, collocations = False)\n",
    "    \n",
    "        wc.generate(text)\n",
    "    \n",
    "        # save wordcloud\n",
    "        wc.to_file('./output/' + file_path)\n",
    "\n",
    "    Parallel(n_jobs=-1)(delayed(generate_wc)(s_theme) for s_theme in themes)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/29 [00:19<?, ?it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "def transform_y(labels):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(labels)\n",
    "\n",
    "    mlb_data = mlb.transform(labels)\n",
    "\n",
    "    print(mlb.classes_)\n",
    "\n",
    "    return mlb_data, mlb\n",
    "\n",
    "def groupby_process(df):\n",
    "    new_df = df.sort_values(['process_id', 'page'])\n",
    "    new_df = new_df.groupby(\n",
    "                ['process_id', 'themes'],\n",
    "                group_keys=False\n",
    "            ).apply(lambda x: x.body.str.cat(sep=' ')).reset_index()\n",
    "    new_df = new_df.rename(index=str, columns={0: \"body\"})\n",
    "    return new_df\n",
    "\n",
    "def get_data(path, preds=None, key=None):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.rename(columns={ 'pages': 'page'})\n",
    "    data = groupby_process(data)\n",
    "    data.themes = data.themes.apply(lambda x: literal_eval(x))\n",
    "    return data\n",
    "\n",
    "corpus_data = get_data('./CSV/train_small.csv')\n",
    "corpus_data.themes = corpus_data.themes.apply(lambda x: list(set(sorted([i if i in THEMES else 0 for i in x]))))\n",
    "X_vecs = corpus_data.body\n",
    "\n",
    "y_vecs, mlb = transform_y(corpus_data.themes)\n",
    "X_vecs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  0   5   6  26  33 139 163 232 313 339 350 406 409 555 589 597 634 660\n",
      " 695 729 766 773 793 800 810 852 895 951 975]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "\n",
    "def generate_wc_freqs(s_theme):\n",
    "    X_vec = X_vecs[[s_theme in theme for theme in corpus_data.themes]]\n",
    "    y_vec = y_vecs[[s_theme in theme for theme in corpus_data.themes]]\n",
    "    tfid_model = TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=True, min_df=0.1, max_features=10000).fit(X_vec, y_vec)\n",
    "    create_wordcloud_freqs(tfid_model.vocabulary_, str(s_theme) + '_freq_wc.png')\n",
    "    # pbar.update(1)\n",
    "\n",
    "def create_wordcloud_freqs(tfidf_freqs, file_path):\n",
    "    # create wordcloud object\n",
    "    wc = WordCloud(background_color=\"white\", width=1280, height=720, max_words=200, collocations = False).generate_from_frequencies(tfidf_freqs)\n",
    "\n",
    "    # save wordcloud\n",
    "    wc.to_file('./output/' + file_path)\n",
    "\n",
    "themes = mlb.classes_\n",
    "\n",
    "_ = Parallel(n_jobs=-1)(delayed(generate_wc_freqs)(s_theme) for s_theme in themes)\n",
    "\n",
    "# generate_wc_freqs(800)\n",
    "\n",
    "# tfidf_freqs = tfid_model.vocabulary_\n",
    "# wc = WordCloud(background_color=\"white\", width=1280, height=720, max_words=200, collocations = False).generate_from_frequencies(tfidf_freqs)\n",
    "# wc.to_file('./output/wc_freqs.png')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# tfid_model = TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=True, min_df=0.1, max_features=10000).fit(X_vec, y_vec)\n",
    "# tfid_model\n",
    "# [[train_data.themes]]\n",
    "# X_train[[800 in theme for theme in train_data.themes]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(X_vecs)\n",
    "\n",
    "for train_index, test_index in kf.split(X_vecs):\n",
    "     # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "     X_train, X_test = X_vecs[train_index], X_vecs[test_index]\n",
    "     y_train, y_test = y_vecs[train_index], y_vecs[test_index]\n",
    "\n",
    "     kfold_model = OneVsRestClassifier(MultinomialNB(alpha=0.001, fit_prior=True), n_jobs=-1).fit(X_train, y_train)\n",
    "     train_score = kfold_model.score(X_train, y_train)\n",
    "     test_score = kfold_model.score(X_test, y_test)\n",
    "     print(\"Train Score: \", train_score, \"Test Score: \", test_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def word_importance_by_theme(classifier, tag, tags_classes, index_to_words):\n",
    "    coefs = classifier.coef_[tags_classes.index(tag)]\n",
    "    sortedWords = [(index_to_words[x], coef) for coef,x in sorted(zip(coefs, range(len(coefs))))]\n",
    "    return sortedWords\n",
    "\n",
    "def generate_wc_freqs(s_theme):\n",
    "    X_vec = X_vecs[[s_theme in theme for theme in corpus_data.themes]]\n",
    "    y_vec = y_vecs[[s_theme in theme for theme in corpus_data.themes]]\n",
    "\n",
    "    tfid_model = TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=True, min_df=0.1, max_features=10000).fit(X_vec, y_vec)\n",
    "    word_coefs = word_importance_by_theme(tfid_model, s_theme, mlb.classes_.tolist(), {i:word for word,i in tfidf.vocabulary_.items()})\n",
    "    \n",
    "    create_wordcloud_freqs(word_coefs, str(s_theme) + '_freq_wc.png')\n",
    "    # pbar.update(1)\n",
    "\n",
    "def create_wordcloud_freqs(tfidf_freqs, file_path):\n",
    "    # create wordcloud object\n",
    "    wc = WordCloud(background_color=\"white\", width=1280, height=720, max_words=200, collocations = False).generate_from_frequencies(tfidf_freqs)\n",
    "\n",
    "    # save wordcloud\n",
    "    wc.to_file('./output/' + file_path)\n",
    "\n",
    "themes = mlb.classes_\n",
    "\n",
    "_ = Parallel(n_jobs=-1)(delayed(generate_wc_freqs)(s_theme) for s_theme in themes)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}